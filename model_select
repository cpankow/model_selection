#!/usr/bin/env python

#######################
### MODEL SELECTION ###
#######################

# --- Import packages --- #
import sys
import argparse
import h5py

import numpy as np
import pandas as pd
import scipy.stats

from populations import bbh_models
from populations import msplot

import emcee
from emcee import PTSampler

VERBOSE=True
MAKE_PLOTS=True

# --- Argument handling --- #
argp = argparse.ArgumentParser()
argp.add_argument("-d", "--dirpath", type=str, required=True, help="Sets the directory path where the models are living. Models must be stored as an hdf file with different keys storing the information about different models. The name of the hdf file is the name of the population (e.g. 'field.hdf' and 'cluster.hdf'.")
argp.add_argument("-m", "--model0", type=str, required=True, help="Sets the 'true' model from which mock observations are drawn. If 'gwobs', will use the actual GW observations.")
argp.add_argument("-gw", "--gwpath", type=str, help="Sets the path for where the GW samples are living. Default=None.")
argp.add_argument("-b", "--beta", nargs="*", help="Chooses the branching fraction. The number provided is the fraction of systems that come from each channel in alphabetical order. Must be set if gwobs not used. Can provide Nchannel values (which must sum to unity) or Nchannel-1 values (which must be less than 1.0, and the last branching fraction is inferred). Default=None.")
argp.add_argument("-n", "--Nobs", type=int, help="Number of mock observations to be taken from the model. Must be set if gwobs not used. Default=None.")
argp.add_argument("-N", "--Nsamps", type=int, default=100, help="Number of samples to be drawn from the posterior distributions and used to construct the KDE models. Default=100.")
argp.add_argument("-s", "--smear", type=str, help="Smear out observations in a mocked up attempt at non-delta function observation posteriors. For GW obs, you can choose to smear using the actual posterior samples ('posterior'), or from a mock gaussian ('gaussian'). For mock observations, you can currently choose 'gaussian' to smear according to the average spread in parameters from current GW observations. Default=None.")
argp.add_argument("-p", "--params", nargs="+", default="mchirp", help="Specifies the parameters you wish to use in the inference. Default=mchirp.")
argp.add_argument("-sm", "--spinmag", type=str, help="Define the spin magnitude distribution you wish to use. Required for using spin parameters in populations where the sping magnitude is not specified. Default is None.")
argp.add_argument("-S", "--save-samples", action="store_true", help="Save all the samples rather than just the summary statistics. Default is False.")
argp.add_argument("-rs", "--random-seed", type=int, help="Use this to set the random seed. Default=None.")
argp.add_argument("--name", type=str, help="Use this as the stem for all file output. Default=None.")
args = argp.parse_args()

# --- Set random seed
if args.random_seed:
    np.random.seed(args.random_seed)

# --- Load in models/kdemodels into dict structure: models[model][channel]
models, kde_models = bbh_models.get_models(args.dirpath, args.params, args.spinmag, weighting=True)
model_names = list(models.keys())
model_names.sort()
channels = list(models[list(models.keys())[0]].keys())
channels.sort()
params = args.params

# see if we're using GW observations
gwobs = True if args.model0=='gwobs' else False

# construct dict that relates submodels to their index number
submodels_dict = {}
for idx, model in enumerate(model_names):
    submodels_dict[idx] = model

if VERBOSE:
    print("Formation channels: " + ", ".join(channels))
    print("Astrophysical models: " + ", ".join(model_names))
    print("Parameters for inference: " + ", ".join(params))
    print("")



# --- Perform some checks to make sure things are compatible

# check to see if the models have the same formation channels
for model in model_names:
    if set(models[model].keys()) != set(channels):
        raise ValueError("Different models have differing formation channels! Check your hdf keys...")

# check that a value of beta is provided if gwobs not specified
if not gwobs:
    if not args.beta:
        raise ValueError("You need to either specify branching fractions or choose to instead use GW observations!")

# check that the true model provided is valid is gwobs not specified
if (args.model0 not in model_names and not gwobs):
    raise ValueError("The true model you specified ({0:s}) is not one of the models in {1:s} directory!".format(args.model0, args.dirpath))

# check that Nobs was specified if not using gwobs
if not gwobs and not args.Nobs:
    raise ValueError("You need to specify and number of observations to be drawn from the 'true' model if not using GW observations!")

# parse the branching ratio values and check for errors
if not gwobs:
    betas = [float(x) for x in args.beta]
    if (len(betas) != len(channels)) and (len(betas) != len(channels)-1):
        raise ValueError("Must specify {0:d} or {1:d} branching fractions, you provided {2:d}!".format(len(channels)-1, len(channels), len(betas)))
    if np.sum(betas) > 1.0:
        raise ValueError("Branching fractions must sum to less than or equal to 1.0, yours sum to {0:0.2f}!".format(np.sum(betas)))
    if (len(betas) == len(channels)) and (np.sum(betas) != 1.0):
        raise ValueError("If you provide the same number of branching fractions as channels, they must sum to exactly 1.0 (yours sum to {0:0.2f})!".format(np.sum(betas)))

    if len(betas) != len(channels):
        betas.append(1.0 - np.sum(betas))

# --- If model0 specified, save relative fractions for each KDE model and store model0
if not gwobs:
    if VERBOSE:
        print("Saving relative fractions and storing true model...\n")

    for model in model_names:
        # since channels are sorted alphabetically, the first channel has branching fraction beta
        for idx, (channel, beta) in enumerate(zip(channels, betas)):
            kde_models[model][channel].rel_frac(beta)
            if (model == args.model0):
                model0 = kde_models[model]
                if VERBOSE:
                    if idx==0:
                        print("'{0:s}' set as true model".format(model))
                        print("  model range:")
                    print("    {0:s} (beta={1:0.2f})".format(channel, beta))
                    for param in args.params:
                        print("      {0:s}: {1:0.2f} - {2:0.2f}".format(param, models[args.model0][channel][param].min(), models[args.model0][channel][param].max()))
                    if idx==len(betas)-1:
                        print("")


# --- Generate observations ([observations, params, samples])
if VERBOSE:
    print("Generating observations...\n")

# Calls gw_obs.py if argument is passed
if gwobs:
    from populations import gw_obs
    model0=None
    obsdata, events = gw_obs.generate_observations(params, args.gwpath, args.Nsamps, args.smear)
    if VERBOSE:
        print("Using the following {0:d} GW observations for inference:".format(len(events)))
        print(*events, sep=', ')
        print("")

else:
    if VERBOSE:
        print("Drawing {0:d} observations from the true model...\n".format(int(args.Nobs)))
    for idx, channel in enumerate(model0):
        # FIXME: the floors and ceilings here are a bit hacky but ensure that we have a total of Nobs samples drawn when Nobs*beta is not an integer
        if idx==0:
            Nobs_per_channel = int(np.floor(args.Nobs * model0[channel]._rel_frac))
            obsdata = model0[channel].generate_observations(Nobs_per_channel, args.Nsamps, args.smear)
        else:
            Nobs_per_channel = int(np.ceil(args.Nobs * model0[channel]._rel_frac))
            obsdata = np.concatenate((obsdata, model0[channel].generate_observations(Nobs_per_channel, args.Nsamps, args.smear)))



# Plot the KDEs and observations
if MAKE_PLOTS==True:
    print("Plotting marginalized KDE models...\n")
    msplot.plot_1D_kdemodels(kde_models, params, obsdata, model0, plot_obs=True, plot_obs_samples=False)


# --- Freeze sample evaluations in each model (This is time consuming, but only needs to be done once for each KDE model so we don't need to recompute p_model(data) over and over again when the observation values aren't going to change. Lower the number of samples per observation to speed this up.)
if VERBOSE:
    print("Freezing sample evaluations in their respective models...")
for model in model_names:
    for channel in channels:
        if VERBOSE:
            print("  {0:s}, {1:s}".format(model, channel))
        kde_models[model][channel].freeze(obsdata)
if VERBOSE:
    print("Done freezing KDE evaluations of observations!\n")





# --- Define the likelihood and prior

def lnp(x):
    """
    Log of the prior. Returns logL of -inf for points outside, uniform within. Is conditional on the sum of the betas being one.
    """
    model = x[0]
    betas_tmp = np.asarray(x[1:])
    betas_tmp = np.append(betas_tmp, 1-np.sum(betas_tmp))

    if np.floor(model) not in range(0, len(model_names)):
        return -np.inf

    if np.any(betas_tmp < 0.0):
        return -np.inf

    if np.sum(betas_tmp) > 1.0:
        return -np.inf

    # Dirchlet distribution prior for betas
    return scipy.stats.dirichlet.logpdf(betas_tmp, _concentration)

def lnlike(x, data):
    """
    Log of the likelihood. Selects on model, then tests beta.
    """
    model_idx = int(np.floor(x[0]))
    model = submodels_dict[model_idx]
    betas_tmp = np.asarray(x[1:])
    betas_tmp = np.append(betas_tmp, 1-np.sum(betas_tmp))

    # Prior
    lp = lnp(x)
    if not np.isfinite(lp):
        return lp

    # Likelihood
    prob = np.zeros(data.shape[0])

    # Iterate over channels in this submodel, return cached values
    for channel, beta in zip(channels, betas_tmp):
        smdl = kde_models[model][channel]
        prob += beta * smdl(data)

    return np.log(prob).sum() + lp




# --- Run the sampler!

# Set up walkers (note that ndim is (Nchannels-1) + 1 for the model index)
ndim = (len(channels)-1) + 1
nwalkers = 16
ntemps = 4
nsteps = 5000

# Set up initial point for the walkers
p0PT = np.empty(shape=(ntemps, nwalkers, ndim))
# branching fraction
_concentration = np.ones(len(channels))
p0PT[:,:,:] = scipy.stats.dirichlet.rvs(_concentration, (p0PT.shape[0], p0PT.shape[1]))
# model index (note that we overwrite one of the betas with the model index; we only use Nchannel-1 betas in the inference because of the implicit constraint that Sum(betas) = 1)
p0PT[:,:,0] = np.random.uniform(0, len(kde_models), size=(ntemps, nwalkers))

# Do the actual sampling
if VERBOSE:
    print("Sampling...")
sampler = PTSampler(ntemps, nwalkers, ndim, lnlike, lnp, loglargs=[obsdata])
for idx, result in enumerate(sampler.sample(p0PT, iterations=nsteps)):
    if VERBOSE:
        if (idx+1) % 50 == 0:
            sys.stderr.write("\r  {0}% (N={1})".format(float(idx+1)*100. / nsteps, idx+1))
if VERBOSE:
    print("\nSampling complete!\n")



# --- Process the chains

# specify number of steps for burnin
burnin = 1000
nsamps_after_burning = (nsteps-burnin)
samples = sampler.chain[:,:,burnin:,:]
lnprb = sampler.lnprobability[:,:,burnin:]

# synthesize last betas, since they sum to unity
last_betas = (1.0-np.sum(samples[...,1:], axis=3))
last_betas = np.expand_dims(last_betas, axis=3)
samples = np.concatenate((samples, last_betas), axis=3)

# Plot samples and histograms of betas per model
if MAKE_PLOTS==True:
    print("Plotting samples...\n")
    msplot.plot_samples(samples, model_names, channels, model0)


# reshape to have all steps from all walkers in a single dimension (note that we added a dim for the last beta that we synthesized)
samples = samples.reshape((ntemps, nwalkers * nsamps_after_burning, ndim+1))
lnprb = lnprb.reshape((ntemps, nwalkers * nsamps_after_burning))

# Parse out the model index in low-T chain
smdl_counts = np.floor(samples[0,:,0]).astype(int)

# Print summary info about the sampling
if VERBOSE:
    print("Sample breakdown:")
    for smdl_idx, smdl in enumerate(model_names):
        counts = len(smdl_counts[smdl_counts == smdl_idx])
        locs = np.where(smdl_counts==smdl_idx)
        if len(locs[0]) > 0:
            beta = samples[0,:,1][locs].mean()
        else:
            beta = np.nan
        print("  Model {0:s} ({1:d}): {2:d} samples, beta={3:0.2f}".format(smdl, smdl_idx, counts, beta))
    print("")



# --- Save samples

if args.save_samples:
    if args.name:
        fname = "output_" + args.name + ".hdf5"
    else:
        fname = "output.hdf5"

    if VERBOSE:
        print("Saving information to '{0:s}'...\n".format(fname))

    hfile = h5py.File(fname, "w")
    bsgrp = hfile.create_group("model_selection")

    # add model0 attribute
    if args.model0=='gwobs':
        info = np.append([str(args.model0)], [*events])
    else:
        info = np.append([str(args.model0)], [*[key+': '+str(model0[key]._rel_frac) for key in model0.keys()]])
    info = [x.encode('utf-8') for x in info]
    bsgrp.attrs["model0_params"] = info

    # add argument attribute
    arguments = sys.argv[1:]
    arguments = [x.encode('utf-8') for x in arguments]
    bsgrp.attrs["args"] = arguments

    # save observations dataset
    bsgrp.create_dataset("observations", data=obsdata)

    # create subgroup for samples
    group = bsgrp.create_group("samples")
    labels = ["model_idx"] + ["beta_%s" % c for c in channels]
    group.attrs["inference_params"] = labels

    # synthesize last beta, since they sum to unity
    for lbl, d in zip(labels, samples.T):
        group.create_dataset(lbl, data=d)
    hfile.close()

